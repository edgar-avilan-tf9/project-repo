{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1w1S8yjcOhVh-082cOAeNnac5WpM1vBhl",
      "authorship_tag": "ABX9TyMYSlIpcPgK91Gv5666uI2h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edgar-avilan-tf9/project-repo/blob/Pix2PixHD/PixHDCity2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f6fDnY4XCTUJ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip /content/drive/MyDrive/GANdataset/gtFine_trainvaltest.zip -d /content/city_scapes\n",
        "!rm /content/city_scapes/license.txt\n",
        "!rm /content/city_scapes/README"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip /content/drive/MyDrive/GANdataset/leftImg8bit_trainvaltest.zip -d /content/city_scapes"
      ],
      "metadata": {
        "id": "18ZUubXuYmLu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/city_scapes/README"
      ],
      "metadata": {
        "id": "n0fiOMIKb_mJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/city_scapes/license.txt"
      ],
      "metadata": {
        "id": "QzYp-cMfcGgo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "fsqoaT8PJZoz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pix2pixHD_utils import *"
      ],
      "metadata": {
        "id": "YmJT03OGOhE2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check for dataset"
      ],
      "metadata": {
        "id": "sbuWoS9QOyEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample image\n",
        "link1= examples3['strasbourg_000001_009097']['label_map']\n",
        "img1=Image.open(link1)"
      ],
      "metadata": {
        "id": "vhRSrN2FOtXI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_width(img, target_width, method):\n",
        "    '''\n",
        "    Function that scales an image to target_width while retaining aspect ratio.\n",
        "    '''\n",
        "    w, h = img.size\n",
        "    if w == target_width: return img\n",
        "    target_height = target_width * h // w\n",
        "    return img.resize((target_width, target_height), method)"
      ],
      "metadata": {
        "id": "kCqNSORRhzph"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda img: scale_width(img1, 50, Image.BICUBIC)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "A7RqhIO4Qanh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_printoptions(linewidth=5000, edgeitems=6)"
      ],
      "metadata": {
        "id": "bNYYCId9Ukg7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_ten=transform(img1)"
      ],
      "metadata": {
        "id": "vIZrredTQteo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_ten.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goISc9I8Q4mj",
        "outputId": "ff398009-bc9c-49e6-909e-bf7ca3c9e6a1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 25, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_suffix = '_leftImg8bit.png'\n",
        "label_suffix = '_gtFine_labelIds.png'\n",
        "inst_suffix = '_gtFine_instanceIds.png'"
      ],
      "metadata": {
        "id": "PCiRs1vhS9WH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_path= '/content/city_scapes'"
      ],
      "metadata": {
        "id": "AehDJldYbmjs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Part 1\n",
        "for root, i_, files in os.walk(main_path):\n",
        "  print(root)\n",
        "  print(i_)\n",
        "  print(files)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFk7k2cpdnw6",
        "outputId": "081e0d57-99fe-41d8-9f78-6bbde12cc933"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/city_scapes\n",
            "['gtFine', 'leftImg8bit']\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(img_suffix)\n",
        "examples3 = {}"
      ],
      "metadata": {
        "id": "aCos2AV3gqpQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for root, _, files in os.walk(main_path):\n",
        "  if 'train' in root:\n",
        "    for f in files:\n",
        "     # if 'train' in f:\n",
        "        if f.endswith(img_suffix):\n",
        "            prefix = f[:-len(img_suffix)]\n",
        "            attr = 'orig_img'\n",
        "        elif f.endswith(label_suffix):\n",
        "            prefix = f[:-len(label_suffix)]\n",
        "            attr = 'label_map'\n",
        "        elif f.endswith(inst_suffix):\n",
        "            prefix = f[:-len(inst_suffix)]\n",
        "            attr = 'inst_map'\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        if prefix not in examples3.keys():\n",
        "            examples3[prefix] = {}\n",
        "        examples3[prefix][attr] = root + '/' + f"
      ],
      "metadata": {
        "id": "15tTtM6ieRfw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples3['strasbourg_000001_009097']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzbygRFViBRS",
        "outputId": "62fa227e-79bf-4a35-d82d-b10b63a72ce3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label_map': '/content/city_scapes/gtFine/train/strasbourg/strasbourg_000001_009097_gtFine_labelIds.png',\n",
              " 'inst_map': '/content/city_scapes/gtFine/train/strasbourg/strasbourg_000001_009097_gtFine_instanceIds.png',\n",
              " 'orig_img': '/content/city_scapes/leftImg8bit/train/strasbourg/strasbourg_000001_009097_leftImg8bit.png'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(examples3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xmke_4ieic0F",
        "outputId": "732f9940-6600-450f-e79e-eb443b52577d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2975"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples4 = list(examples3.values())"
      ],
      "metadata": {
        "id": "O5tEp5IJjjJ2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples4[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ete1yY4j_Kb",
        "outputId": "369c2c09-36a5-4ffc-deaf-5b63b6fec7c8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'inst_map': '/content/city_scapes/gtFine/train/erfurt/erfurt_000059_000019_gtFine_instanceIds.png',\n",
              " 'label_map': '/content/city_scapes/gtFine/train/erfurt/erfurt_000059_000019_gtFine_labelIds.png',\n",
              " 'orig_img': '/content/city_scapes/leftImg8bit/train/erfurt/erfurt_000059_000019_leftImg8bit.png'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up Dataload functions"
      ],
      "metadata": {
        "id": "7NpXO8gViIm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CityscapesDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    CityscapesDataset Class\n",
        "    Values:\n",
        "        paths: (a list of) paths to load examples from, a list or string\n",
        "        target_width: the size of image widths for resizing, a scalar\n",
        "        n_classes: the number of object classes, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, paths, target_width=1024, n_classes=35):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Collect list of examples\n",
        "        self.examples = {}\n",
        "        if type(paths) == str:\n",
        "            self.load_examples_from_dir(paths)\n",
        "        elif type(paths) == list:\n",
        "            for path in paths:\n",
        "                self.load_examples_from_dir(path)\n",
        "        else:\n",
        "            raise ValueError('`paths` should be a single path or list of paths')\n",
        "\n",
        "        self.examples = list(self.examples.values())\n",
        "        assert all(len(example) == 3 for example in self.examples)\n",
        "\n",
        "        # Initialize transforms for the real color image\n",
        "        self.img_transforms = transforms.Compose([\n",
        "            transforms.Lambda(lambda img: scale_width(img, target_width, Image.BICUBIC)),\n",
        "            transforms.Lambda(lambda img: np.array(img)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ])\n",
        "\n",
        "        # Initialize transforms for semantic label and instance maps\n",
        "        self.map_transforms = transforms.Compose([\n",
        "            transforms.Lambda(lambda img: scale_width(img, target_width, Image.NEAREST)),\n",
        "            transforms.Lambda(lambda img: np.array(img)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def load_examples_from_dir(self, abs_path):\n",
        "        '''\n",
        "        Given a folder of examples, this function returns a list of paired examples.\n",
        "        '''\n",
        "        assert os.path.isdir(abs_path)\n",
        "\n",
        "        img_suffix = '_leftImg8bit.png'\n",
        "        label_suffix = '_gtFine_labelIds.png'\n",
        "        inst_suffix = '_gtFine_instanceIds.png'\n",
        "\n",
        "        for root, _, files in os.walk(abs_path):\n",
        "          if 'train' in root:\n",
        "              for f in files:\n",
        "                  if f.endswith(img_suffix):\n",
        "                      prefix = f[:-len(img_suffix)]\n",
        "                      attr = 'orig_img'\n",
        "                  elif f.endswith(label_suffix):\n",
        "                      prefix = f[:-len(label_suffix)]\n",
        "                      attr = 'label_map'\n",
        "                  elif f.endswith(inst_suffix):\n",
        "                      prefix = f[:-len(inst_suffix)]\n",
        "                      attr = 'inst_map'\n",
        "                  else:\n",
        "                      continue\n",
        "\n",
        "                  if prefix not in self.examples.keys():\n",
        "                      self.examples[prefix] = {}\n",
        "                  self.examples[prefix][attr] = root + '/' + f\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.examples[idx]\n",
        "\n",
        "        # Load image and maps\n",
        "        img = Image.open(example['orig_img']).convert('RGB') # color image: (3, 512, 1024)\n",
        "        inst = Image.open(example['inst_map'])               # instance map: (512, 1024)\n",
        "        label = Image.open(example['label_map'])             # semantic label map: (512, 1024)\n",
        "\n",
        "        # Apply corresponding transforms\n",
        "        img = self.img_transforms(img)\n",
        "        inst = self.map_transforms(inst)\n",
        "        label = self.map_transforms(label).long() * 255\n",
        "\n",
        "        # Convert labels to one-hot vectors\n",
        "        label = torch.zeros(self.n_classes, img.shape[1], img.shape[2]).scatter_(0, label, 1.0).to(img.dtype)\n",
        "\n",
        "        # Convert instance map to instance boundary map\n",
        "        bound = torch.ByteTensor(inst.shape).zero_()\n",
        "        bound[:, :, 1:] = bound[:, :, 1:] | (inst[:, :, 1:] != inst[:, :, :-1])\n",
        "        bound[:, :, :-1] = bound[:, :, :-1] | (inst[:, :, 1:] != inst[:, :, :-1])\n",
        "        bound[:, 1:, :] = bound[:, 1:, :] | (inst[:, 1:, :] != inst[:, :-1, :])\n",
        "        bound[:, :-1, :] = bound[:, :-1, :] | (inst[:, 1:, :] != inst[:, :-1, :])\n",
        "        bound = bound.to(img.dtype)\n",
        "\n",
        "        return (img, label, inst, bound)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        imgs, labels, insts, bounds = [], [], [], []\n",
        "        for (x, l, i, b) in batch:\n",
        "            imgs.append(x)\n",
        "            labels.append(l)\n",
        "            insts.append(i)\n",
        "            bounds.append(b)\n",
        "        return (\n",
        "            torch.stack(imgs, dim=0),\n",
        "            torch.stack(labels, dim=0),\n",
        "            torch.stack(insts, dim=0),\n",
        "            torch.stack(bounds, dim=0),\n",
        "        )"
      ],
      "metadata": {
        "id": "VGr4aiIciMEW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "n_classes = 35                  # total number of object classes\n",
        "rgb_channels = n_features = 3\n",
        "device = 'cuda'\n",
        "train_dir = ['/content/city_scapes']\n",
        "epochs = 200                    # total number of train epochs\n",
        "decay_after = 100               # number of epochs with constant lr\n",
        "lr = 0.0002\n",
        "betas = (0.5, 0.999)\n",
        "\n",
        "def lr_lambda(epoch):\n",
        "    ''' Function for scheduling learning '''\n",
        "    return 1. if epoch < decay_after else 1 - float(epoch - decay_after) / (epochs - decay_after)\n",
        "\n",
        "def weights_init(m):\n",
        "    ''' Function for initializing all model weights '''\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        nn.init.normal_(m.weight, 0., 0.02)\n",
        "\n",
        "loss_fn = Loss(device=device)\n",
        "\n",
        "## Phase 1: Low Resolution (1024 x 512)\n",
        "dataloader1 = DataLoader(\n",
        "    CityscapesDataset(train_dir, target_width=1024, n_classes=n_classes),\n",
        "    collate_fn=CityscapesDataset.collate_fn, batch_size=1, shuffle=True, drop_last=False, pin_memory=True,\n",
        ")\n",
        "encoder = Encoder(rgb_channels, n_features).to(device).apply(weights_init)\n",
        "generator1 = GlobalGenerator(n_classes + n_features + 1, rgb_channels).to(device).apply(weights_init)\n",
        "discriminator1 = MultiscaleDiscriminator(n_classes + 1 + rgb_channels, n_discriminators=2).to(device).apply(weights_init)\n",
        "\n",
        "g1_optimizer = torch.optim.Adam(list(generator1.parameters()) + list(encoder.parameters()), lr=lr, betas=betas)\n",
        "d1_optimizer = torch.optim.Adam(list(discriminator1.parameters()), lr=lr, betas=betas)\n",
        "g1_scheduler = torch.optim.lr_scheduler.LambdaLR(g1_optimizer, lr_lambda)\n",
        "d1_scheduler = torch.optim.lr_scheduler.LambdaLR(d1_optimizer, lr_lambda)\n",
        "\n",
        "\n",
        "## Phase 2: High Resolution (2048 x 1024)\n",
        "dataloader2 = DataLoader(\n",
        "    CityscapesDataset(train_dir, target_width=2048, n_classes=n_classes),\n",
        "    collate_fn=CityscapesDataset.collate_fn, batch_size=1, shuffle=True, drop_last=False, pin_memory=True,\n",
        ")\n",
        "generator2 = LocalEnhancer(n_classes + n_features + 1, rgb_channels).to(device).apply(weights_init)\n",
        "discriminator2 = MultiscaleDiscriminator(n_classes + 1 + rgb_channels).to(device).apply(weights_init)\n",
        "\n",
        "g2_optimizer = torch.optim.Adam(list(generator2.parameters()) + list(encoder.parameters()), lr=lr, betas=betas)\n",
        "d2_optimizer = torch.optim.Adam(list(discriminator2.parameters()), lr=lr, betas=betas)\n",
        "g2_scheduler = torch.optim.lr_scheduler.LambdaLR(g2_optimizer, lr_lambda)\n",
        "d2_scheduler = torch.optim.lr_scheduler.LambdaLR(d2_optimizer, lr_lambda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Zyd9bK_iVT6",
        "outputId": "08fee880-fd91-4454-ae7f-e4e786a9ae02"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flkKC_gaoZU4",
        "outputId": "44bb6d64-8aad-43a0-fc66-78971f8facf7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7cf7138e0850>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loop"
      ],
      "metadata": {
        "id": "hAnpkkGxnRt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parse torch version for autocast\n",
        "# ######################################################\n",
        "version = torch.__version__\n",
        "version = tuple(int(n) for n in version.split('.')[:-1])\n",
        "has_autocast = version >= (1, 6)\n",
        "# ######################################################\n",
        "\n",
        "def show_tensor_images(image_tensor):\n",
        "    '''\n",
        "    Function for visualizing images: Given a tensor of images, number of images, and\n",
        "    size per image, plots and prints the images in an uniform grid.\n",
        "    '''\n",
        "    image_tensor = (image_tensor + 1) / 2\n",
        "    image_unflat = image_tensor.detach().cpu()\n",
        "    image_grid = make_grid(image_unflat[:1], nrow=1)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    plt.show()\n",
        "\n",
        "def train(dataloader, models, optimizers, schedulers, device, tr_name):\n",
        "    encoder, generator, discriminator = models\n",
        "    g_optimizer, d_optimizer = optimizers\n",
        "    g_scheduler, d_scheduler = schedulers\n",
        "\n",
        "    cur_step = 0\n",
        "    display_step = 500\n",
        "\n",
        "    mean_g_loss = 0.0\n",
        "    mean_d_loss = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training epoch\n",
        "        for (x_real, labels, insts, bounds) in tqdm(dataloader, position=0):\n",
        "            x_real = x_real.to(device)\n",
        "            labels = labels.to(device)\n",
        "            insts = insts.to(device)\n",
        "            bounds = bounds.to(device)\n",
        "\n",
        "            # Enable autocast to FP16 tensors (new feature since torch==1.6.0)\n",
        "            # If you're running older versions of torch, comment this out\n",
        "            # and use NVIDIA apex for mixed/half precision training\n",
        "            if has_autocast:\n",
        "                with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
        "                    g_loss, d_loss, x_fake = loss_fn(\n",
        "                        x_real, labels, insts, bounds, encoder, generator, discriminator\n",
        "                    )\n",
        "            else:\n",
        "                g_loss, d_loss, x_fake = loss_fn(\n",
        "                    x_real, labels, insts, bounds, encoder, generator, discriminator\n",
        "                )\n",
        "\n",
        "            g_optimizer.zero_grad()\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "            d_optimizer.zero_grad()\n",
        "            d_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "            mean_g_loss += g_loss.item() / display_step\n",
        "            mean_d_loss += d_loss.item() / display_step\n",
        "\n",
        "            if cur_step % display_step == 0 and cur_step > 0:\n",
        "                print('Step {}: Generator loss: {:.5f}, Discriminator loss: {:.5f}'\n",
        "                      .format(cur_step, mean_g_loss, mean_d_loss))\n",
        "                show_tensor_images(x_fake.to(x_real.dtype))\n",
        "                show_tensor_images(x_real)\n",
        "                mean_g_loss = 0.0\n",
        "                mean_d_loss = 0.0\n",
        "            cur_step += 1\n",
        "        torch.save(model_customs.state_dict(), tr_name)\n",
        "\n",
        "        g_scheduler.step()\n",
        "        d_scheduler.step()"
      ],
      "metadata": {
        "id": "pnu3MM-9j4ag"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kick train"
      ],
      "metadata": {
        "id": "vqLyVj1epEBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Phase 1: Low Resolution\n",
        "#######################################################################\n",
        "train(\n",
        "    dataloader1,\n",
        "    [encoder, generator1, discriminator1],\n",
        "    [g1_optimizer, d1_optimizer],\n",
        "    [g1_scheduler, d1_scheduler],\n",
        "    device,'CityModel1.pt'\n",
        ")\n",
        "\n",
        "\n",
        "# Phase 2: High Resolution\n",
        "#######################################################################\n",
        "# Update global generator in local enhancer with trained\n",
        "generator2.g1 = generator1.g1\n",
        "\n",
        "# Freeze encoder and wrap to support high-resolution inputs/outputs\n",
        "def freeze(encoder):\n",
        "    encoder.eval()\n",
        "    for p in encoder.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    @torch.jit.script\n",
        "    def forward(x, inst):\n",
        "        x = F.interpolate(x, scale_factor=0.5, recompute_scale_factor=True)\n",
        "        inst = F.interpolate(inst.float(), scale_factor=0.5, recompute_scale_factor=True)\n",
        "        feat = encoder(x, inst.int())\n",
        "        return F.interpolate(feat, scale_factor=2.0, recompute_scale_factor=True)\n",
        "    return forward\n",
        "\n",
        "train(\n",
        "    dataloader2,\n",
        "    [freeze(encoder), generator2, discriminator2],\n",
        "    [g2_optimizer, d2_optimizer],\n",
        "    [g2_scheduler, d2_scheduler],\n",
        "    device,'CityModel2.pt'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evvc1bYXo813",
        "outputId": "0c4f4d9d-63a5-41ca-a60d-f3d4be6c2459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|â–‹         | 222/2975 [02:59<35:51,  1.28it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tFQamfOCp5HM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}